{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f92943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "import os\n",
    "import plotly.io as pio\n",
    "from optbinning import OptimalBinning\n",
    "\n",
    "\n",
    "# T·∫Øt c·∫£nh b√°o\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# C·∫•u h√¨nh hi·ªÉn th·ªã\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65845bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 1. LOAD DATA\n",
    "# # ============================================================\n",
    "# print(\">>> [1/4] LOADING DATA...\")\n",
    "\n",
    "# # ƒê∆∞·ªùng d·∫´n (gi·ªØ nguy√™n c·ªßa b·∫°n)\n",
    "# data_path = r'C:\\Users\\PC\\Documents\\GitHub\\Khoa-luan\\EBM'\n",
    "\n",
    "# try:\n",
    "#     X_train = pd.read_parquet(f'{data_path}/X_train.parquet')\n",
    "#     y_train = pd.read_parquet(f'{data_path}/y_train.parquet').squeeze()\n",
    "\n",
    "#     X_val = pd.read_parquet(f'{data_path}/X_oos.parquet')\n",
    "#     y_val = pd.read_parquet(f'{data_path}/y_oos.parquet').squeeze()\n",
    "\n",
    "#     X_test = pd.read_parquet(f'{data_path}/X_oot.parquet')\n",
    "#     y_test = pd.read_parquet(f'{data_path}/y_oot.parquet').squeeze()\n",
    "    \n",
    "#     print(f\"   Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file Parquet.\")\n",
    "#     # D·ª´ng ch∆∞∆°ng tr√¨nh t·∫°i ƒë√¢y n·∫øu l·ªói (trong Notebook th√¨ d√πng raise)\n",
    "#     raise\n",
    "\n",
    "# # ============================================================\n",
    "# # 2. CONFIG FEATURE LISTS\n",
    "# # ============================================================\n",
    "# # List bi·∫øn Numerical\n",
    "# cols_num = [\n",
    "#     'BASE_AUM', 'TUOI', 'INCOME', 'CBAL', 'AFLIMT_AVG', 'LTV', \n",
    "#     'N_AVG_DEPOSIT_12M', 'UTILIZATION_RATE', 'AMT_CASH_ADVANCE_12M',\n",
    "#     'PCT_PAYMENT_TO_BALANCE', 'AVG_DAYS_PAST_DUE', 'DTI_RATIO', \n",
    "#     'LIMIT_TO_INCOME', 'AMT_VAR_6M', 'CBAL_SHORTTERM_LOAN', \n",
    "#     'CBAL_LONGTERM_LOAN', 'RATE_AVG', 'MAX_DPD_12M', 'AVG_OD_DPD_12M', \n",
    "#     'N_AVG_OVERDUE_CBAL_12M', 'CNT_CREDIT_CARDS', 'CNT_MIN_PAY_6M', \n",
    "#     'CNT_OTHER_PRODUCTS', 'CNT_DPD_30PLUS_6M', 'MOB', 'DURATION_MAX', \n",
    "#     'REMAINING_DURATION_MAX', 'TIME_TO_OP_MAX', 'MAX_NHOMNOCIC'\n",
    "# ]\n",
    "\n",
    "# # List bi·∫øn Categorical\n",
    "# cols_cat = [\n",
    "#     'C_GIOITINH', 'TTHONNHAN', 'SEGMENT', \n",
    "#     'OCCUPATION_TYPE', 'PURCOD_MAX',      \n",
    "#     'NHANVIENBIDV', 'FLAG_SALARY_ACC', 'FLAG_DEPOSIT', 'HAS_LONGTERM_LOAN'\n",
    "# ]\n",
    "\n",
    "# # C√°c c·ªôt c·∫ßn lo·∫°i b·ªè (ID, Target n·∫øu c√≥ trong X, v.v.)\n",
    "# # L∆∞u √Ω: X_train th∆∞·ªùng kh√¥ng c√≥ Target, nh∆∞ng c·ª© ƒë·ªÉ ƒë√¢y cho an to√†n\n",
    "# exclude_cols = [\"SAMPLE_TYPE\", \"SOCIF\", \"REF_MONTH\", \"BAD_FLAG\", \"BAD_NEXT_12M\"] \n",
    "\n",
    "# # L·∫•y danh s√°ch feature cu·ªëi c√πng c√≥ trong X_train\n",
    "# final_features = [c for c in X_train.columns if c not in exclude_cols]\n",
    "\n",
    "# print(f\">>> [2/4] PREPROCESSING {len(final_features)} FEATURES...\")\n",
    "# feature_types_list = []\n",
    "# missing_cols = []\n",
    "# X_train_ebm = X_train[final_features].copy()\n",
    "# # ============================================================\n",
    "# # 3. √âP KI·ªÇU D·ªÆ LI·ªÜU (QUAN TR·ªåNG)\n",
    "# # ============================================================\n",
    "# # L∆∞u √Ω: Ph·∫£i d√πng ƒë√∫ng t√™n bi·∫øn X_train, X_val, X_test thay v√¨ train/oos/oot\n",
    "\n",
    "# # ============================================================\n",
    "# # 4. CHU·∫®N H√ìA D·ªÆ LI·ªÜU & TRAIN (PH∆Ø∆†NG PH√ÅP PANDAS CATEGORY)\n",
    "# # ============================================================\n",
    "# print(\">>> [3/4] PREPARING DATA WITH PANDAS CATEGORY DTYPE...\")\n",
    "\n",
    "# # 1. T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω\n",
    "# X_train_ebm = X_train[final_features].copy()\n",
    "\n",
    "# # 2. X·ª≠ l√Ω √©p ki·ªÉu d·ªØ li·ªáu\n",
    "# # Thay v√¨ d√πng feature_types list, ta √©p th·∫≥ng v√†o dtype c·ªßa DataFrame\n",
    "# for col in final_features:\n",
    "#     # --- X·ª¨ L√ù BI·∫æN CATEGORICAL ---\n",
    "#     if col in cols_cat:\n",
    "#         # B∆∞·ªõc 1: FillNA -> String\n",
    "#         # B∆∞·ªõc 2: √âp sang 'category' (ƒê√¢y l√† ch√¨a kh√≥a ƒë·ªÉ EBM t·ª± hi·ªÉu)\n",
    "#         X_train_ebm[col] = X_train_ebm[col].fillna(\"MISSING\").astype(str).astype('category')\n",
    "        \n",
    "#     # --- X·ª¨ L√ù BI·∫æN NUMERICAL ---\n",
    "#     elif col in cols_num:\n",
    "#         X_train_ebm[col] = pd.to_numeric(X_train_ebm[col], errors='coerce')\n",
    "        \n",
    "#     # --- X·ª¨ L√ù BI·∫æN S√ìT (M·∫∑c ƒë·ªãnh ƒë∆∞a v·ªÅ Num) ---\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è C·∫£nh b√°o: C·ªôt '{col}' ch∆∞a ƒë∆∞·ª£c ph√¢n lo·∫°i -> √âp v·ªÅ s·ªë.\")\n",
    "#         X_train_ebm[col] = pd.to_numeric(X_train_ebm[col], errors='coerce')\n",
    "\n",
    "# print(\"‚úÖ ƒê√£ chu·∫©n h√≥a dtype: C√°c bi·∫øn Cate ƒë√£ chuy·ªÉn th√†nh 'category', Num th√†nh 'float/int'.\")\n",
    "\n",
    "# # ============================================================\n",
    "# # 5. TRAIN EBM\n",
    "# # ============================================================\n",
    "# print(\">>> [4/4] TRAINING EBM...\")\n",
    "\n",
    "# # L∆ØU √ù QUAN TR·ªåNG: \n",
    "# # B·ªè tham s·ªë 'feature_types' ƒëi. \n",
    "# # EBM s·∫Ω t·ª± ƒë·ªçc dtype 'category' c·ªßa Pandas v√† x·ª≠ l√Ω ƒë√∫ng nh∆∞ √Ω b·∫°n.\n",
    "# ebm = ExplainableBoostingClassifier(\n",
    "#     random_state=42,\n",
    "#     interactions=0, \n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "\n",
    "# # Train v·ªõi d·ªØ li·ªáu ƒë√£ √©p ki·ªÉu category\n",
    "# ebm.fit(X_train_ebm, y_train)\n",
    "\n",
    "# print(\"‚úÖ TRAINING XONG! KH√îNG C√íN L·ªñI.\")\n",
    "\n",
    "# # Show Dashboard\n",
    "# ebm_global = ebm.explain_global()\n",
    "# show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddbfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. T·∫°o th∆∞ m·ª•c ch·ª©a ·∫£nh\n",
    "# output_folder = \"EBM_Autoscale_Plots\"\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "\n",
    "# # L·∫•y danh s√°ch t√™n bi·∫øn\n",
    "# features = ebm_global.feature_names\n",
    "# print(f\">>> B·∫Øt ƒë·∫ßu xu·∫•t {len(features)} bi·ªÉu ƒë·ªì (Ch·∫ø ƒë·ªô Autoscale)...\")\n",
    "\n",
    "# # 2. V√≤ng l·∫∑p xu·∫•t ·∫£nh\n",
    "# for i, feature_name in enumerate(features):\n",
    "#     try:\n",
    "#         # L·∫•y Figure g·ªëc t·ª´ EBM\n",
    "#         fig = ebm_global.visualize(i)\n",
    "        \n",
    "#         # --- B∆Ø·ªöC QUAN TR·ªåNG NH·∫§T: B·∫¨T AUTOSCALE ---\n",
    "#         # L·ªánh n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác b·∫°n b·∫•m n√∫t \"Autoscale\" tr√™n giao di·ªán\n",
    "#         fig.update_yaxes(autorange=True)\n",
    "        \n",
    "#         # T√πy ch·ªânh th√™m (Title, Size) cho ƒë·∫πp\n",
    "#         fig.update_layout(\n",
    "#             title=f\"EBM Plot (Autoscaled): {feature_name}\",\n",
    "#             width=1200, height=600,\n",
    "#             font=dict(size=14),\n",
    "#             # Th√™m d√≤ng n√†y ƒë·ªÉ tr·ª•c Y hi·ªán r√µ l∆∞·ªõi grid cho d·ªÖ nh√¨n\n",
    "#             yaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "#         )\n",
    "        \n",
    "#         # L∆∞u file\n",
    "#         safe_name = feature_name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "#         file_path = f\"{output_folder}/{i}_{safe_name}.png\"\n",
    "        \n",
    "#         fig.write_image(file_path, scale=2)\n",
    "#         print(f\"   [OK] {feature_name}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"   [L·ªñI] Bi·∫øn {feature_name}: {e}\")\n",
    "\n",
    "# print(f\"\\n‚úÖ Xong! M·ªü th∆∞ m·ª•c '{output_folder}' ƒë·ªÉ xem th√†nh qu·∫£.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fafbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing = ['CBAL', 'LTV', 'ULTILIZATION_RATE', 'CNT_CREDIT_CARDS', 'PCT_PAYMENT_TO_BALANCE', 'CNT_MIN_PAY_6M', 'DTI_RATIO', 'MOB', 'LIMIT_TO_INCOME', 'AMT_VAR_6M', 'DURATION_MAX', 'REMAINING_DURATION_MAX', 'MAX_DPD_12M', 'MAC_NHOMNOCIC', 'N_AVG__OVERDUE_CBAL_12M']\n",
    "# decreasing = ['BASE_AUM', 'TUOI', 'INCOME', 'AFLIMT_AVG', 'N_AVG_DEPOSIT_12M', 'AMT_CASH_ADVANCE_12M', 'AVG_DAYS_PAST_DUE', 'CNT_OTHER_PRODUCTS', 'CBAL_SHORTTERM_LOAN', \"CBAL_LONGTERM_LOAN\", 'CNT_DPD_30PLUS_6M', 'TIME_TO_OP_MAX', 'RATE_AVG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb066026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [1/4] LOADING DATA...\n",
      "   Train: (1137807, 28) | Val: (300317, 28) | Test: (302113, 28)\n",
      ">>> [2/4] PREPROCESSING 28 FEATURES...\n",
      ">>> [3/4] PREPARING DATA WITH PANDAS CATEGORY DTYPE...\n",
      "‚úÖ ƒê√£ chu·∫©n h√≥a dtype: C√°c bi·∫øn Cate ƒë√£ chuy·ªÉn th√†nh 'category', Num th√†nh 'float/int'.\n",
      ">>> [4/4] TRAINING EBM...\n",
      "   K·∫øt qu·∫£ Baseline:\n",
      "   [TRAIN-Base] AUC: 0.8716 | GINI: 0.7433\n",
      "   [OOS-Base] AUC: 0.8786 | GINI: 0.7573\n",
      "   [OOT-Base] AUC: 0.8775 | GINI: 0.7551\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2300040830576/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2300040830576/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================\n",
    "print(\">>> [1/4] LOADING DATA...\")\n",
    "\n",
    "def calculate_gini(model, X, y, label=\"\"):\n",
    "    prob = model.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y, prob)\n",
    "    gini = 2 * auc - 1\n",
    "    if label:\n",
    "        print(f\"   [{label}] AUC: {auc:.4f} | GINI: {gini:.4f}\")\n",
    "    return gini\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n (gi·ªØ nguy√™n c·ªßa b·∫°n)\n",
    "data_path = r'C:\\Users\\PC\\Documents\\GitHub\\Khoa-luan\\EBM'\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_parquet(f'{data_path}/X_train.parquet')\n",
    "    y_train = pd.read_parquet(f'{data_path}/y_train.parquet').squeeze()\n",
    "\n",
    "    X_val = pd.read_parquet(f'{data_path}/X_oos.parquet')\n",
    "    y_val = pd.read_parquet(f'{data_path}/y_oos.parquet').squeeze()\n",
    "\n",
    "    X_test = pd.read_parquet(f'{data_path}/X_oot.parquet')\n",
    "    y_test = pd.read_parquet(f'{data_path}/y_oot.parquet').squeeze()\n",
    "    \n",
    "    print(f\"   Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file Parquet.\")\n",
    "    # D·ª´ng ch∆∞∆°ng tr√¨nh t·∫°i ƒë√¢y n·∫øu l·ªói (trong Notebook th√¨ d√πng raise)\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# 2. CONFIG FEATURE LISTS\n",
    "# ============================================================\n",
    "# List bi·∫øn Numerical\n",
    "cols_num = [\n",
    "    'BASE_AUM', 'TUOI', 'INCOME', 'CBAL', 'AFLIMT_AVG', 'LTV', \n",
    "    'N_AVG_DEPOSIT_12M', 'UTILIZATION_RATE', 'AMT_CASH_ADVANCE_12M',\n",
    "    'PCT_PAYMENT_TO_BALANCE', 'AVG_DAYS_PAST_DUE', 'DTI_RATIO', \n",
    "    'LIMIT_TO_INCOME', 'AMT_VAR_6M', 'CBAL_SHORTTERM_LOAN', \n",
    "    'CBAL_LONGTERM_LOAN', 'RATE_AVG', 'MAX_DPD_12M', 'AVG_OD_DPD_12M', \n",
    "    'N_AVG_OVERDUE_CBAL_12M', 'CNT_CREDIT_CARDS', 'CNT_MIN_PAY_6M', \n",
    "    'CNT_OTHER_PRODUCTS', 'CNT_DPD_30PLUS_6M', 'MOB', 'DURATION_MAX', \n",
    "    'REMAINING_DURATION_MAX', 'TIME_TO_OP_MAX', 'MAX_NHOMNOCIC'\n",
    "]\n",
    "\n",
    "# List bi·∫øn Categorical\n",
    "cols_cat = [\n",
    "    'C_GIOITINH', 'TTHONNHAN', 'SEGMENT', \n",
    "    'OCCUPATION_TYPE', 'PURCOD_MAX',      \n",
    "    'NHANVIENBIDV', 'FLAG_SALARY_ACC', 'FLAG_DEPOSIT', 'HAS_LONGTERM_LOAN'\n",
    "]\n",
    "\n",
    "# C√°c c·ªôt c·∫ßn lo·∫°i b·ªè (ID, Target n·∫øu c√≥ trong X, v.v.)\n",
    "# L∆∞u √Ω: X_train th∆∞·ªùng kh√¥ng c√≥ Target, nh∆∞ng c·ª© ƒë·ªÉ ƒë√¢y cho an to√†n\n",
    "exclude_cols = [\"SAMPLE_TYPE\", \"SOCIF\", \"REF_MONTH\", \"BAD_FLAG\", \"BAD_NEXT_12M\"] \n",
    "\n",
    "# L·∫•y danh s√°ch feature cu·ªëi c√πng c√≥ trong X_train\n",
    "final_features = [c for c in X_train.columns if c not in exclude_cols]\n",
    "print(f\">>> [2/4] PREPROCESSING {len(final_features)} FEATURES...\")\n",
    "feature_types_list = []\n",
    "missing_cols = []\n",
    "X_train_ebm = X_train[final_features].copy()\n",
    "# ============================================================\n",
    "# 3. √âP KI·ªÇU D·ªÆ LI·ªÜU (QUAN TR·ªåNG)\n",
    "# ============================================================\n",
    "# L∆∞u √Ω: Ph·∫£i d√πng ƒë√∫ng t√™n bi·∫øn X_train, X_val, X_test thay v√¨ train/oos/oot\n",
    "\n",
    "# ============================================================\n",
    "# 4. CHU·∫®N H√ìA D·ªÆ LI·ªÜU & TRAIN (PH∆Ø∆†NG PH√ÅP PANDAS CATEGORY)\n",
    "# ============================================================\n",
    "print(\">>> [3/4] PREPARING DATA WITH PANDAS CATEGORY DTYPE...\")\n",
    "\n",
    "# 1. T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω\n",
    "X_train_ebm = X_train[final_features].copy()\n",
    "\n",
    "# 2. X·ª≠ l√Ω √©p ki·ªÉu d·ªØ li·ªáu\n",
    "# Thay v√¨ d√πng feature_types list, ta √©p th·∫≥ng v√†o dtype c·ªßa DataFrame\n",
    "for col in final_features:\n",
    "    # --- X·ª¨ L√ù BI·∫æN CATEGORICAL ---\n",
    "    if col in cols_cat:\n",
    "        # B∆∞·ªõc 1: FillNA -> String\n",
    "        # B∆∞·ªõc 2: √âp sang 'category' (ƒê√¢y l√† ch√¨a kh√≥a ƒë·ªÉ EBM t·ª± hi·ªÉu)\n",
    "        X_train_ebm[col] = X_train_ebm[col].fillna(\"MISSING\").astype(str).astype('category')\n",
    "        \n",
    "    # --- X·ª¨ L√ù BI·∫æN NUMERICAL ---\n",
    "    elif col in cols_num:\n",
    "        X_train_ebm[col] = pd.to_numeric(X_train_ebm[col], errors='coerce')\n",
    "        \n",
    "    # --- X·ª¨ L√ù BI·∫æN S√ìT (M·∫∑c ƒë·ªãnh ƒë∆∞a v·ªÅ Num) ---\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o: C·ªôt '{col}' ch∆∞a ƒë∆∞·ª£c ph√¢n lo·∫°i -> √âp v·ªÅ s·ªë.\")\n",
    "        X_train_ebm[col] = pd.to_numeric(X_train_ebm[col], errors='coerce')\n",
    "\n",
    "print(\"‚úÖ ƒê√£ chu·∫©n h√≥a dtype: C√°c bi·∫øn Cate ƒë√£ chuy·ªÉn th√†nh 'category', Num th√†nh 'float/int'.\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. TRAIN EBM\n",
    "# ============================================================\n",
    "print(\">>> [4/4] TRAINING EBM...\")\n",
    "increasing_cols = [\n",
    "    'CBAL', 'LTV', 'UTILIZATION_RATE', # ƒê√£ s·ª≠a ULTILIZATION -> UTILIZATION\n",
    "    'CNT_CREDIT_CARDS', 'PCT_PAYMENT_TO_BALANCE', 'CNT_MIN_PAY_6M', \n",
    "    'DTI_RATIO', 'MOB', 'LIMIT_TO_INCOME', 'AMT_VAR_6M', \n",
    "    'DURATION_MAX', 'REMAINING_DURATION_MAX', 'MAX_DPD_12M', \n",
    "    'MAX_NHOMNOCIC', # ƒê√£ s·ª≠a MAC -> MAX\n",
    "    'N_AVG_OVERDUE_CBAL_12M' # ƒê√£ s·ª≠a N_AVG__ -> N_AVG_ (b·ªè 1 d·∫•u g·∫°ch)\n",
    "]\n",
    "\n",
    "decreasing_cols = [\n",
    "    'BASE_AUM', 'TUOI', 'INCOME', 'AFLIMT_AVG', 'N_AVG_DEPOSIT_12M', \n",
    "    'AMT_CASH_ADVANCE_12M', 'AVG_DAYS_PAST_DUE', 'CNT_OTHER_PRODUCTS', \n",
    "    'CBAL_SHORTTERM_LOAN', \"CBAL_LONGTERM_LOAN\", 'CNT_DPD_30PLUS_6M', \n",
    "    'TIME_TO_OP_MAX', 'RATE_AVG'\n",
    "]\n",
    "\n",
    "# 2. Map v√†o Dictionary\n",
    "constraints_list = []\n",
    "\n",
    "# G√°n +1 cho nh√≥m Increasing\n",
    "for col in X_train_ebm.columns:\n",
    "    # N·∫øu l√† bi·∫øn Categorical (ƒë√£ √©p ki·ªÉu category) -> B·∫Øt bu·ªôc l√† 0\n",
    "    # (Ki·ªÉm tra xem c·ªôt c√≥ ph·∫£i d·∫°ng category kh√¥ng)\n",
    "    if pd.api.types.is_categorical_dtype(X_train_ebm[col]):\n",
    "        constraints_list.append(0)\n",
    "        \n",
    "    # N·∫øu l√† bi·∫øn s·ªë c·∫ßn √©p TƒÉng (+1)\n",
    "    elif col in increasing_cols:\n",
    "        constraints_list.append(1)\n",
    "        \n",
    "    # N·∫øu l√† bi·∫øn s·ªë c·∫ßn √©p Gi·∫£m (-1)\n",
    "    elif col in decreasing_cols:\n",
    "        constraints_list.append(-1)\n",
    "    \n",
    "\n",
    "# L∆ØU √ù QUAN TR·ªåNG: \n",
    "# B·ªè tham s·ªë 'feature_types' ƒëi. \n",
    "# EBM s·∫Ω t·ª± ƒë·ªçc dtype 'category' c·ªßa Pandas v√† x·ª≠ l√Ω ƒë√∫ng nh∆∞ √Ω b·∫°n.\n",
    "ebm = ExplainableBoostingClassifier(\n",
    "    random_state=42,\n",
    "    monotone_constraints=constraints_list,\n",
    "    interactions=0, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train v·ªõi d·ªØ li·ªáu ƒë√£ √©p ki·ªÉu category\n",
    "ebm.fit(X_train_ebm, y_train)\n",
    "\n",
    "print(\"   K·∫øt qu·∫£ Baseline:\")\n",
    "gini_train = calculate_gini(ebm, X_train, y_train, \"TRAIN-Base\")\n",
    "gini_base_oos = calculate_gini(ebm, X_val, y_val, \"OOS-Base\")\n",
    "gini_base_oot = calculate_gini(ebm, X_test, y_test, \"OOT-Base\")\n",
    "\n",
    "# Show Dashboard\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e10fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. T·∫°o th∆∞ m·ª•c ch·ª©a ·∫£nh\n",
    "# output_folder = \"EBM_Autoscale_Plots_mono\"\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "\n",
    "# # L·∫•y danh s√°ch t√™n bi·∫øn\n",
    "# features = ebm_global.feature_names\n",
    "# print(f\">>> B·∫Øt ƒë·∫ßu xu·∫•t {len(features)} bi·ªÉu ƒë·ªì (Ch·∫ø ƒë·ªô Autoscale)...\")\n",
    "\n",
    "# # 2. V√≤ng l·∫∑p xu·∫•t ·∫£nh\n",
    "# for i, feature_name in enumerate(features):\n",
    "#     try:\n",
    "#         # L·∫•y Figure g·ªëc t·ª´ EBM\n",
    "#         fig = ebm_global.visualize(i)\n",
    "        \n",
    "#         # --- B∆Ø·ªöC QUAN TR·ªåNG NH·∫§T: B·∫¨T AUTOSCALE ---\n",
    "#         # L·ªánh n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác b·∫°n b·∫•m n√∫t \"Autoscale\" tr√™n giao di·ªán\n",
    "#         fig.update_yaxes(autorange=True)\n",
    "        \n",
    "#         # T√πy ch·ªânh th√™m (Title, Size) cho ƒë·∫πp\n",
    "#         fig.update_layout(\n",
    "#             title=f\"EBM Plot (Autoscaled): {feature_name}\",\n",
    "#             width=1200, height=600,\n",
    "#             font=dict(size=14),\n",
    "#             # Th√™m d√≤ng n√†y ƒë·ªÉ tr·ª•c Y hi·ªán r√µ l∆∞·ªõi grid cho d·ªÖ nh√¨n\n",
    "#             yaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "#         )\n",
    "        \n",
    "#         # L∆∞u file\n",
    "#         safe_name = feature_name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "#         file_path = f\"{output_folder}/{i}_{safe_name}.png\"\n",
    "        \n",
    "#         fig.write_image(file_path, scale=2)\n",
    "#         print(f\"   [OK] {feature_name}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"   [L·ªñI] Bi·∫øn {feature_name}: {e}\")\n",
    "\n",
    "# print(f\"\\n‚úÖ Xong! M·ªü th∆∞ m·ª•c '{output_folder}' ƒë·ªÉ xem th√†nh qu·∫£.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cfe376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë bi·∫øn trong m√¥ h√¨nh: 28\n",
      "                   Feature  Importance\n",
      "26             MAX_DPD_12M    1.040961\n",
      "5                      LTV    0.643910\n",
      "27  N_AVG_OVERDUE_CBAL_12M    0.301791\n",
      "14               DTI_RATIO    0.235779\n",
      "6        N_AVG_DEPOSIT_12M    0.201965\n",
      "7          FLAG_SALARY_ACC    0.184082\n",
      "4                   INCOME    0.168187\n",
      "3                     TUOI    0.143377\n",
      "19     CBAL_SHORTTERM_LOAN    0.063732\n",
      "12          CNT_MIN_PAY_6M    0.060575\n",
      "20      CBAL_LONGTERM_LOAN    0.058966\n",
      "8             FLAG_DEPOSIT    0.038714\n",
      "17         LIMIT_TO_INCOME    0.037543\n",
      "24                RATE_AVG    0.027796\n",
      "13       AVG_DAYS_PAST_DUE    0.015640\n",
      "11  PCT_PAYMENT_TO_BALANCE    0.005984\n",
      "15                     MOB    0.005569\n",
      "18              AMT_VAR_6M    0.005389\n",
      "25              PURCOD_MAX    0.004451\n",
      "0               C_GIOITINH    0.004421\n",
      "21       CNT_DPD_30PLUS_6M    0.004295\n",
      "1                TTHONNHAN    0.003832\n",
      "22         OCCUPATION_TYPE    0.003148\n",
      "23            DURATION_MAX    0.002968\n",
      "10    AMT_CASH_ADVANCE_12M    0.001384\n",
      "9         CNT_CREDIT_CARDS    0.001212\n",
      "16      CNT_OTHER_PRODUCTS    0.000959\n",
      "2             NHANVIENBIDV    0.000617\n"
     ]
    }
   ],
   "source": [
    "# L·∫•y t√™n bi·∫øn v√† ƒë·ªô quan tr·ªçng t∆∞∆°ng ·ª©ng\n",
    "feature_names = ebm.feature_names_in_\n",
    "importances = ebm.term_importances()\n",
    "\n",
    "# T·∫°o dataframe ƒë·ªÉ d·ªÖ nh√¨n\n",
    "df_imp = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# S·∫Øp x·∫øp t·ª´ cao xu·ªëng th·∫•p\n",
    "df_imp = df_imp.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# In ra to√†n b·ªô danh s√°ch\n",
    "print(f\"T·ªïng s·ªë bi·∫øn trong m√¥ h√¨nh: {len(df_imp)}\")\n",
    "print(df_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc32f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [1/4] FILTERING FEATURES...\n",
      ">>> [2/4] PREPARING DATA TYPES...\n",
      "‚úÖ ƒê√£ √©p ki·ªÉu xong (Category & Numeric).\n",
      ">>> [3/4] CONFIGURING CONSTRAINTS...\n",
      ">>> [4/4] TRAINING EBM (19 VARIABLES)...\n",
      "‚úÖ TRAINING TH√ÄNH C√îNG!\n",
      "\n",
      "---------------- K·∫æT QU·∫¢ ----------------\n",
      "   [TRAIN (19 Vars)] AUC: 0.8716 | GINI: 0.7433\n",
      "   [OOS (19 Vars)] AUC: 0.8788 | GINI: 0.7576\n",
      "   [OOT (19 Vars)] AUC: 0.8776 | GINI: 0.7552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2300039833360/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2300039833360/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. CH·ªêT DANH S√ÅCH 19 BI·∫æN QUAN TR·ªåNG\n",
    "# ============================================================\n",
    "print(\">>> [1/4] FILTERING FEATURES...\")\n",
    "\n",
    "# Danh s√°ch 19 bi·∫øn \"Golden Features\"\n",
    "selected_features = [\n",
    "    # --- Nh√≥m R·ªßi ro Cao (+1) ---\n",
    "    'LTV', 'MAX_DPD_12M', 'N_AVG_OVERDUE_CBAL_12M',\n",
    "    'DTI_RATIO', 'CNT_MIN_PAY_6M', \n",
    "    'LIMIT_TO_INCOME',\n",
    "    \n",
    "    # --- Nh√≥m T·ªët/T√†i s·∫£n (-1) ---\n",
    "     'INCOME', 'N_AVG_DEPOSIT_12M', \n",
    "    'CBAL_SHORTTERM_LOAN', 'CBAL_LONGTERM_LOAN', 'RATE_AVG',\n",
    "    \n",
    "    # --- Nh√≥m Kh√°c (0) ---\n",
    "    'TUOI', 'FLAG_SALARY_ACC', 'FLAG_DEPOSIT'\n",
    "]\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a nh√≥m Cate v√† Num\n",
    "cols_cat = ['FLAG_SALARY_ACC', 'FLAG_DEPOSIT']\n",
    "cols_num = [c for c in selected_features if c not in cols_cat]\n",
    "\n",
    "# ============================================================\n",
    "# 2. CHU·∫®N H√ìA D·ªÆ LI·ªÜU (TRAIN, VAL, TEST)\n",
    "# ============================================================\n",
    "print(\">>> [2/4] PREPARING DATA TYPES...\")\n",
    "\n",
    "# T·∫°o b·∫£n sao g·ªçn nh·∫π ch·ªâ ch·ª©a 19 c·ªôt\n",
    "X_train_ebm = X_train[selected_features].copy()\n",
    "X_val_ebm   = X_val[selected_features].copy()\n",
    "X_test_ebm  = X_test[selected_features].copy()\n",
    "\n",
    "# H√†m √©p ki·ªÉu th·ªëng nh·∫•t\n",
    "def convert_types(df):\n",
    "    for col in selected_features:\n",
    "        if col in cols_cat:\n",
    "            # √âp v·ªÅ category ƒë·ªÉ EBM t·ª± hi·ªÉu\n",
    "            df[col] = df[col].fillna(\"MISSING\").astype(str).astype('category')\n",
    "        else:\n",
    "            # √âp v·ªÅ s·ªë\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# √Åp d·ª•ng cho c·∫£ 3 t·∫≠p (ƒë·ªÉ t√≠nh Gini OOS/OOT kh√¥ng b·ªã l·ªói)\n",
    "X_train_ebm = convert_types(X_train_ebm)\n",
    "X_val_ebm   = convert_types(X_val_ebm)\n",
    "X_test_ebm  = convert_types(X_test_ebm)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ √©p ki·ªÉu xong (Category & Numeric).\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. C·∫§U H√åNH √âP CHI·ªÄU (MONOTONICITY)\n",
    "# ============================================================\n",
    "print(\">>> [3/4] CONFIGURING CONSTRAINTS...\")\n",
    "\n",
    "# Nh√≥m TƒÉng (+1): Gi√° tr·ªã c√†ng cao -> R·ªßi ro c√†ng cao\n",
    "increasing_cols = [\n",
    "    'LTV', 'MAX_NHOMNOCIC', 'MAX_DPD_12M', 'N_AVG_OVERDUE_CBAL_12M',\n",
    "    'DTI_RATIO', 'UTILIZATION_RATE', 'CBAL', 'CNT_MIN_PAY_6M', \n",
    "    'LIMIT_TO_INCOME' \n",
    "]\n",
    "\n",
    "# Nh√≥m Gi·∫£m (-1): Gi√° tr·ªã c√†ng cao -> R·ªßi ro c√†ng th·∫•p (T·ªët)\n",
    "decreasing_cols = [\n",
    "    'BASE_AUM', 'INCOME', 'AFLIMT_AVG', 'N_AVG_DEPOSIT_12M',\n",
    "    'CBAL_SHORTTERM_LOAN', 'CBAL_LONGTERM_LOAN', 'RATE_AVG'\n",
    "]\n",
    "\n",
    "# T·∫°o list constraints kh·ªõp 100% v·ªõi th·ª© t·ª± c·ªôt trong X_train_ebm\n",
    "constraints_list = []\n",
    "for col in X_train_ebm.columns:\n",
    "    if pd.api.types.is_categorical_dtype(X_train_ebm[col]):\n",
    "        constraints_list.append(0)   # Bi·∫øn Cate -> 0\n",
    "    elif col in increasing_cols:\n",
    "        constraints_list.append(1)   # √âp tƒÉng\n",
    "    elif col in decreasing_cols:\n",
    "        constraints_list.append(-1)  # √âp gi·∫£m\n",
    "    else:\n",
    "        constraints_list.append(0)   # C√≤n l·∫°i (TUOI) -> 0\n",
    "\n",
    "# ============================================================\n",
    "# 4. TRAIN & EVALUATE\n",
    "# ============================================================\n",
    "print(\">>> [4/4] TRAINING EBM (19 VARIABLES)...\")\n",
    "\n",
    "ebm = ExplainableBoostingClassifier(\n",
    "    random_state=42,\n",
    "    monotone_constraints=constraints_list, # List chu·∫©n\n",
    "    interactions=0, \n",
    "    n_jobs=-1,\n",
    "    outer_bags=8,\n",
    "    inner_bags=0\n",
    ")\n",
    "\n",
    "# Train\n",
    "ebm.fit(X_train_ebm, y_train)\n",
    "print(\"‚úÖ TRAINING TH√ÄNH C√îNG!\")\n",
    "\n",
    "# T√≠nh Gini\n",
    "print(\"\\n---------------- K·∫æT QU·∫¢ ----------------\")\n",
    "calculate_gini(ebm, X_train_ebm, y_train, \"TRAIN\")\n",
    "calculate_gini(ebm, X_val_ebm, y_val, \"OOS\")\n",
    "calculate_gini(ebm, X_test_ebm, y_test, \"OOT\")\n",
    "\n",
    "# Show Dashboard\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce3c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [4/6] CREATE MASTER PD-DATAFRAME...\n",
      "‚Üí DF created: (1740237, 3)\n",
      "  DATA_TYPE  y        PD\n",
      "0     TRAIN  0  0.015252\n",
      "1     TRAIN  1  0.200559\n",
      "2     TRAIN  0  0.128738\n",
      "3     TRAIN  0  0.118896\n",
      "4     TRAIN  0  0.099548\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n>>> [4/6] CREATE MASTER PD-DATAFRAME...\")\n",
    "\n",
    "# Predict PD\n",
    "pd_train = ebm.predict_proba(X_train)[:, 1]\n",
    "pd_oos   = ebm.predict_proba(X_val)[:, 1]\n",
    "pd_oot   = ebm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# T·∫°o DF cho t·ª´ng t·∫≠p\n",
    "df_train = pd.DataFrame({\n",
    "    \"DATA_TYPE\": \"TRAIN\",\n",
    "    \"y\": y_train,\n",
    "    \"PD\": pd_train\n",
    "})\n",
    "\n",
    "df_oos = pd.DataFrame({\n",
    "    \"DATA_TYPE\": \"OOS\",\n",
    "    \"y\": y_val,\n",
    "    \"PD\": pd_oos\n",
    "})\n",
    "\n",
    "df_oot = pd.DataFrame({\n",
    "    \"DATA_TYPE\": \"OOT\",\n",
    "    \"y\": y_test,\n",
    "    \"PD\": pd_oot\n",
    "})\n",
    "\n",
    "# G·ªôp l·∫°i\n",
    "df = pd.concat([df_train, df_oos, df_oot], ignore_index=True)\n",
    "\n",
    "print(\"‚Üí DF created:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e83112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [5/6] CONVERT PD ‚Üí SCORE\n",
      "  DATA_TYPE  y        PD       SCORE\n",
      "0     TRAIN  0  0.015252  833.131738\n",
      "1     TRAIN  1  0.200559  752.776352\n",
      "2     TRAIN  0  0.128738  768.050556\n",
      "3     TRAIN  0  0.118896  770.669436\n",
      "4     TRAIN  0  0.099548  776.420916\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n>>> [5/6] CONVERT PD ‚Üí SCORE\")\n",
    "\n",
    "PDO = 20\n",
    "ODDS0 = 50\n",
    "SCORE0 = 600\n",
    "\n",
    "Factor = PDO / np.log(2)\n",
    "Offset = SCORE0 + Factor * np.log(ODDS0)\n",
    "\n",
    "def pd_to_score(pd):\n",
    "    pd = np.clip(pd, 1e-6, 1 - 1e-6)\n",
    "    log_odds = np.log(pd / (1 - pd))\n",
    "    return Offset - Factor * log_odds\n",
    "\n",
    "df[\"SCORE\"] = pd_to_score(df[\"PD\"])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62320b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [6/6] FIT OPT BINNING ON TRAIN...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n>>> [6/6] FIT OPT BINNING ON TRAIN...\")\n",
    "\n",
    "optb = OptimalBinning(\n",
    "    name=\"rating_scale\",\n",
    "    dtype=\"numerical\",\n",
    "    monotonic_trend=\"descending\",  # Risk cao ‚Üí Score th·∫•p\n",
    "    max_n_bins=10\n",
    ")\n",
    "\n",
    "optb.fit(df[df[\"DATA_TYPE\"]==\"TRAIN\"][\"SCORE\"], df[df[\"DATA_TYPE\"]==\"TRAIN\"][\"y\"])\n",
    "df[\"BIN\"] = optb.transform(df[\"SCORE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d1a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RATING DONE\n",
      "  DATA_TYPE  y        PD       SCORE       BIN RATING\n",
      "0     TRAIN  0  0.015252  833.131738  1.607355    AA-\n",
      "1     TRAIN  1  0.200559  752.776352 -1.030027    BBB\n",
      "2     TRAIN  0  0.128738  768.050556 -0.591242     A-\n",
      "3     TRAIN  0  0.118896  770.669436 -0.591242     A-\n",
      "4     TRAIN  0  0.099548  776.420916 -0.591242     A-\n"
     ]
    }
   ],
   "source": [
    "rating_labels = [\"AAA\",\"AA+\",\"AA\",\"AA-\",\"A+\",\"A\",\"A-\",\"BBB\",\"BB\",\"B\"]\n",
    "\n",
    "# Trung b√¨nh PD t·ª´ng BIN ‚Üí ƒë·ªÉ x·∫øp th·ª© t·ª± h·∫°ng\n",
    "bin_mean_pd = df.groupby(\"BIN\")[\"PD\"].mean().sort_values()\n",
    "sorted_bins = bin_mean_pd.index.tolist()\n",
    "\n",
    "rating_map = {b: rating_labels[i] for i, b in enumerate(sorted_bins)}\n",
    "\n",
    "df[\"RATING\"] = df[\"BIN\"].map(rating_map)\n",
    "\n",
    "print(\"RATING DONE\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a1b0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> CUTPOINTS (PD Thresholds):\n",
      "[0.00134957 0.00393775 0.00944629 0.02339609 0.05875492 0.09367731\n",
      " 0.14628037 0.22327075 0.32770728]\n",
      "\n",
      ">>> RATING SUMMARY TABLE:\n",
      "RATING     SCORE_RANGE  COUNT    PCT  MEAN_PD DEFAULT_RATE\n",
      "   AAA 903.50 ‚Üí 953.29 121246  6.97% 0.000901     0.000792\n",
      "   AA+ 872.53 ‚Üí 903.50 221304 12.72% 0.002580     0.002883\n",
      "    AA 847.12 ‚Üí 872.53 303698 17.45% 0.006296     0.006421\n",
      "   AA- 820.55 ‚Üí 847.12 315623 18.14% 0.015484     0.016054\n",
      "    A+ 792.91 ‚Üí 820.55 265124 15.23% 0.036965     0.035994\n",
      "     A 778.36 ‚Üí 792.91 104977  6.03% 0.075155     0.081018\n",
      "    A- 763.78 ‚Üí 778.36 120787  6.94% 0.118950     0.129029\n",
      "   BBB 748.85 ‚Üí 763.78 121921  7.01% 0.181447     0.183947\n",
      "    BB 733.61 ‚Üí 748.85  86759  4.99% 0.268824     0.254925\n",
      "     B 649.05 ‚Üí 733.61  78798  4.53% 0.460983     0.455595\n"
     ]
    }
   ],
   "source": [
    "# --- TI·∫æP N·ªêI T·ª™ CODE C·ª¶A B·∫†N ---\n",
    "\n",
    "# 1. ƒê·ªãnh nghƒ©a th·ª© t·ª± h·∫°ng (ƒë·ªÉ s·∫Øp x·∫øp b·∫£ng cho ƒë√∫ng chu·∫©n t·ª´ T·ªët -> X·∫•u)\n",
    "rating_order = [\"AAA\", \"AA+\", \"AA\", \"AA-\", \"A+\", \"A\", \"A-\", \"BBB\", \"BB\", \"B\"]\n",
    "\n",
    "# 2. Groupby ƒë·ªÉ t√≠nh to√°n c√°c ch·ªâ s·ªë\n",
    "# L∆∞u √Ω: C·ªôt target c·ªßa b·∫°n trong df t√™n l√† \"y\"\n",
    "summary = df.groupby(\"RATING\").agg({\n",
    "    \"SCORE\": [\"min\", \"max\"],      # ƒê·ªÉ l·∫•y kho·∫£ng ƒëi·ªÉm\n",
    "    \"PD\": [\"mean\", \"max\"],        # Mean PD ƒë·ªÉ so s√°nh, Max PD ƒë·ªÉ t√¨m ƒëi·ªÉm c·∫Øt (Cutpoint)\n",
    "    \"y\": [\"count\", \"mean\"]        # Count = S·ªë l∆∞·ª£ng, Mean = Default Rate th·ª±c t·∫ø\n",
    "}).reset_index()\n",
    "\n",
    "# L√†m ph·∫≥ng MultiIndex c·ªßa c·ªôt sau khi group\n",
    "summary.columns = ['RATING', 'MIN_SCORE', 'MAX_SCORE', 'MEAN_PD', 'MAX_PD_BIN', 'COUNT', 'DEFAULT_RATE']\n",
    "\n",
    "# 3. S·∫Øp x·∫øp b·∫£ng theo th·ª© t·ª± Rating (AAA -> B)\n",
    "summary['RATING'] = pd.Categorical(summary['RATING'], categories=rating_order, ordered=True)\n",
    "summary = summary.sort_values('RATING').reset_index(drop=True)\n",
    "\n",
    "# 4. T√≠nh to√°n c√°c c·ªôt hi·ªÉn th·ªã (Format l·∫°i cho ƒë·∫πp)\n",
    "# T√≠nh % t·ª∑ tr·ªçng\n",
    "total_obs = summary['COUNT'].sum()\n",
    "summary['PCT'] = summary['COUNT'] / total_obs\n",
    "\n",
    "# T·∫°o c·ªôt Score Range \"Min -> Max\"\n",
    "# L∆∞u √Ω: V√¨ Score v√† PD ng∆∞·ª£c chi·ªÅu (Score cao = PD th·∫•p), \n",
    "# n√™n h·∫°ng AAA s·∫Ω c√≥ Score cao nh·∫•t. Ta hi·ªÉn th·ªã t·ª´ Min -> Max cho thu·∫≠n m·∫Øt.\n",
    "summary['SCORE_RANGE'] = summary.apply(lambda x: f\"{x['MIN_SCORE']:.2f} ‚Üí {x['MAX_SCORE']:.2f}\", axis=1)\n",
    "\n",
    "# 5. Tr√≠ch xu·∫•t Cutpoints (ƒêi·ªÉm c·∫Øt PD)\n",
    "# ƒêi·ªÉm c·∫Øt ch√≠nh l√† PD l·ªõn nh·∫•t (Max PD) c·ªßa t·ª´ng h·∫°ng.\n",
    "# Kh√°ch h√†ng c√≥ PD <= Cutpoint c·ªßa AAA s·∫Ω v√†o AAA.\n",
    "# Ta l·∫•y values[:-1] ƒë·ªÉ b·ªè qua ƒëi·ªÉm c·∫Øt cu·ªëi c√πng (c·ªßa h·∫°ng B, th∆∞·ªùng l√† 1.0)\n",
    "cutpoints = summary['MAX_PD_BIN'].values[:-1]\n",
    "\n",
    "# 6. S·∫Øp x·∫øp l·∫°i c·ªôt ƒë·ªÉ in ra b·∫£ng Final (gi·ªëng m·∫´u b·∫°n y√™u c·∫ßu)\n",
    "final_table = summary[['RATING', 'SCORE_RANGE', 'COUNT', 'PCT', 'MEAN_PD', 'DEFAULT_RATE']]\n",
    "\n",
    "# --- IN K·∫æT QU·∫¢ ---\n",
    "print(\"\\n>>> CUTPOINTS (PD Thresholds):\")\n",
    "print(cutpoints)\n",
    "\n",
    "print(\"\\n>>> RATING SUMMARY TABLE:\")\n",
    "# Format hi·ªÉn th·ªã d·∫°ng b·∫£ng ƒë·∫πp\n",
    "pd.set_option('display.float_format', '{:.6f}'.format) # ƒê·ªÉ nh√¨n r√µ s·ªë PD nh·ªè\n",
    "print(final_table.to_string(index=False, formatters={\n",
    "    'PCT': '{:.2%}'.format,         # D·∫°ng % (vd: 8.42%)\n",
    "    'MEAN_PD': '{:.6f}'.format,     # Gi·ªØ nguy√™n s·ªë th·∫≠p ph√¢n ho·∫∑c {:.2%} t√πy b·∫°n\n",
    "    'DEFAULT_RATE': '{:.6f}'.format \n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c476092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> EXTRACTING SCORECARD...\n",
      "                  Feature        Type  Bin_Index      Range/Value  Score_Raw  Score_Points     Points\n",
      "0  INTERCEPT (Base Score)        Base         -1              All  -3.760922   -108.517271 109.000000\n",
      "1                     LTV  Continuous          0         < 2.7556   0.000000           NaN  -0.000000\n",
      "2                     LTV  Continuous          1  2.7556 - 3.0518  -1.335303           NaN  39.000000\n",
      "3                     LTV  Continuous          2  3.0518 - 3.2593  -1.335303           NaN  39.000000\n",
      "4                     LTV  Continuous          3  3.2593 - 3.4235  -1.335303           NaN  39.000000\n",
      "5                     LTV  Continuous          4  3.4235 - 3.5535  -1.335303           NaN  39.000000\n",
      "6                     LTV  Continuous          5  3.5535 - 3.6802  -1.333843           NaN  38.000000\n",
      "7                     LTV  Continuous          6  3.6802 - 3.7878  -1.333843           NaN  38.000000\n",
      "8                     LTV  Continuous          7  3.7878 - 3.8845  -1.332336           NaN  38.000000\n",
      "9                     LTV  Continuous          8  3.8845 - 3.9691  -1.332336           NaN  38.000000\n",
      "‚úÖ ƒê√£ xu·∫•t ma tr·∫≠n t√≠nh ƒëi·ªÉm ra file 'EBM_Scorecard_Matrix.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT EBM SCORECARD TO EXCEL\n",
    "# ============================================================\n",
    "print(\">>> EXTRACTING SCORECARD...\")\n",
    "\n",
    "scorecard_data = []\n",
    "\n",
    "# 1. L·∫•y ƒëi·ªÉm c∆° s·ªü (Intercept)\n",
    "# ƒê√¢y l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho m·ªçi kh√°ch h√†ng\n",
    "intercept = ebm.intercept_[0] if hasattr(ebm, \"intercept_\") else ebm.intercept_\n",
    "scorecard_data.append({\n",
    "    \"Feature\": \"INTERCEPT (Base Score)\",\n",
    "    \"Type\": \"Base\",\n",
    "    \"Bin_Index\": -1,\n",
    "    \"Range/Value\": \"All\",\n",
    "    \"Score_Raw\": intercept,\n",
    "    \"Score_Points\": intercept * (20 / np.log(2)) # Scale sang ƒëi·ªÉm h·ªì s∆° n·∫øu c·∫ßn\n",
    "})\n",
    "\n",
    "# 2. Duy·ªát qua t·ª´ng bi·∫øn ƒë·ªÉ l·∫•y b·∫£ng ƒëi·ªÉm\n",
    "# ebm.term_names_ : T√™n bi·∫øn\n",
    "# ebm.term_scores_: ƒêi·ªÉm s·ªë t∆∞∆°ng ·ª©ng t·ª´ng bin\n",
    "# ebm.bins_       : C√°c ƒëi·ªÉm c·∫Øt (Cutoffs) c·ªßa bin\n",
    "for i, feature_name in enumerate(ebm.term_names_):\n",
    "    \n",
    "    # L·∫•y th√¥ng tin c·ªßa bi·∫øn th·ª© i\n",
    "    # L∆∞u √Ω: EBM l∆∞u scores v√† bins theo index\n",
    "    scores = ebm.term_scores_[i]\n",
    "    \n",
    "    # X·ª≠ l√Ω bi·∫øn Continuous (S·ªë) vs Categorical (Ch·ªØ)\n",
    "    feature_type = ebm.feature_types_in_[i]\n",
    "    \n",
    "    if feature_type == 'continuous':\n",
    "        # V·ªõi bi·∫øn s·ªë, ta c√≥ c√°c ƒëi·ªÉm c·∫Øt (cutoffs)\n",
    "        # bins[i] ch·ª©a c√°c ng∆∞·ª°ng c·∫Øt. V√≠ d·ª•: [20, 30, 40]\n",
    "        # -> C√°c kho·∫£ng l√†: (-inf, 20], (20, 30], (30, 40], (40, +inf)\n",
    "        # S·ªë l∆∞·ª£ng scores lu√¥n nhi·ªÅu h∆°n s·ªë l∆∞·ª£ng cutoffs 1 ƒë∆°n v·ªã\n",
    "        cutoffs = ebm.bins_[i][0] \n",
    "        \n",
    "        # Bin ƒë·∫ßu ti√™n (-inf -> cutoff 0)\n",
    "        scorecard_data.append({\n",
    "            \"Feature\": feature_name,\n",
    "            \"Type\": \"Continuous\",\n",
    "            \"Bin_Index\": 0,\n",
    "            \"Range/Value\": f\"< {cutoffs[0]:.4f}\",\n",
    "            \"Score_Raw\": scores[0]\n",
    "        })\n",
    "        \n",
    "        # C√°c bin ·ªü gi·ªØa\n",
    "        for k in range(1, len(cutoffs)):\n",
    "            scorecard_data.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Type\": \"Continuous\",\n",
    "                \"Bin_Index\": k,\n",
    "                \"Range/Value\": f\"{cutoffs[k-1]:.4f} - {cutoffs[k]:.4f}\",\n",
    "                \"Score_Raw\": scores[k]\n",
    "            })\n",
    "            \n",
    "        # Bin cu·ªëi c√πng (> cutoff cu·ªëi)\n",
    "        scorecard_data.append({\n",
    "            \"Feature\": feature_name,\n",
    "            \"Type\": \"Continuous\",\n",
    "            \"Bin_Index\": len(cutoffs),\n",
    "            \"Range/Value\": f\">= {cutoffs[-1]:.4f}\",\n",
    "            \"Score_Raw\": scores[-1]\n",
    "        })\n",
    "        \n",
    "    elif feature_type == 'nominal': # Categorical\n",
    "        # V·ªõi bi·∫øn Cate, bins[i][0] ch·ª©a danh s√°ch c√°c gi√° tr·ªã (labels)\n",
    "        labels = ebm.bins_[i][0]\n",
    "        \n",
    "        # EBM mapping t·ª´ng label v√†o score\n",
    "        # L∆∞u √Ω: C√≥ th·ªÉ c√≥ bin \"Unseen/Missing\" t√πy version, nh∆∞ng th∆∞·ªùng kh·ªõp index\n",
    "        for k, label in enumerate(labels):\n",
    "             scorecard_data.append({\n",
    "                \"Feature\": feature_name,\n",
    "                \"Type\": \"Categorical\",\n",
    "                \"Bin_Index\": k,\n",
    "                \"Range/Value\": str(label),\n",
    "                \"Score_Raw\": scores[k]\n",
    "            })\n",
    "\n",
    "# 3. T·∫°o DataFrame\n",
    "df_scorecard = pd.DataFrame(scorecard_data)\n",
    "\n",
    "# 4. T√≠nh ƒëi·ªÉm quy ƒë·ªïi (Scaled Score)\n",
    "# C√¥ng th·ª©c Score = Offset + Factor * (-Raw_Score) \n",
    "# (L∆∞u √Ω d·∫•u: EBM ra Raw Score d∆∞∆°ng l√† R·ªßi ro cao -> C·∫ßn tr·ª´ ƒëi)\n",
    "# ·ªû ƒë√¢y ta t√≠nh \"Contribution\" (ƒêi·ªÉm ƒë√≥ng g√≥p): Points = -Raw * Factor\n",
    "factor = 20 / np.log(2)\n",
    "df_scorecard[\"Points\"] = -df_scorecard[\"Score_Raw\"] * factor\n",
    "df_scorecard[\"Points\"] = df_scorecard[\"Points\"].round(0) # L√†m tr√≤n ƒëi·ªÉm\n",
    "\n",
    "# 5. Xu·∫•t ra Excel\n",
    "print(df_scorecard.head(10))\n",
    "df_scorecard.to_excel(\"EBM_Scorecard_Matrix.xlsx\", index=False)\n",
    "print(\"‚úÖ ƒê√£ xu·∫•t ma tr·∫≠n t√≠nh ƒëi·ªÉm ra file 'EBM_Scorecard_Matrix.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47dae873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìä B√ÅO C√ÅO CH·ªà S·ªê N√ÇNG CAO CHO EBM\n",
      "========================================\n",
      ">>> REPORT FOR: OOT DATASET (T∆∞∆°ng lai)\n",
      "   ‚Ä¢ KS Statistic:       0.6156 (61.56%)\n",
      "   ‚Ä¢ Brier Score:        0.046040\n",
      "   ‚Ä¢ Capture Rate @ 10%: 51.52% (T√≥m ƒë∆∞·ª£c bao nhi√™u Bad trong top 10% r·ªßi ro nh·∫•t)\n",
      "   ‚Ä¢ Capture Rate @ 20%: 76.07%\n",
      "------------------------------------------------------------\n",
      ">>> STABILITY (PSI - Train vs OOT): 0.0287\n",
      "   ‚úÖ PSI < 0.1: M√¥ h√¨nh r·∫•t ·ªïn ƒë·ªãnh.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# ============================================================\n",
    "# H√ÄM T√çNH TO√ÅN C√ÅC CH·ªà S·ªê N√ÇNG CAO\n",
    "# ============================================================\n",
    "def calculate_advanced_metrics(model, X, y, dataset_label=\"\"):\n",
    "    # 1. D·ª± b√°o x√°c su·∫•t\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # 2. T√≠nh KS Statistic\n",
    "    # T√°ch x√°c su·∫•t c·ªßa nh√≥m Good (0) v√† Bad (1)\n",
    "    prob_good = y_prob[y == 0]\n",
    "    prob_bad  = y_prob[y == 1]\n",
    "    ks_stat, p_value = ks_2samp(prob_good, prob_bad)\n",
    "    \n",
    "    # 3. T√≠nh Brier Score (C√†ng th·∫•p c√†ng t·ªët)\n",
    "    brier = brier_score_loss(y, y_prob)\n",
    "    \n",
    "    # 4. T√≠nh Bad Capture Rate @ Top 10% & 20%\n",
    "    # T·∫°o DataFrame t·∫°m ƒë·ªÉ sort\n",
    "    df_temp = pd.DataFrame({'y': y, 'prob': y_prob})\n",
    "    df_temp = df_temp.sort_values(by='prob', ascending=False)\n",
    "    \n",
    "    total_bads = df_temp['y'].sum()\n",
    "    n_rows = len(df_temp)\n",
    "    \n",
    "    # Top 10%\n",
    "    top_10_idx = int(n_rows * 0.1)\n",
    "    bads_in_top_10 = df_temp.iloc[:top_10_idx]['y'].sum()\n",
    "    capture_rate_10 = bads_in_top_10 / total_bads if total_bads > 0 else 0\n",
    "    \n",
    "    # Top 20%\n",
    "    top_20_idx = int(n_rows * 0.2)\n",
    "    bads_in_top_20 = df_temp.iloc[:top_20_idx]['y'].sum()\n",
    "    capture_rate_20 = bads_in_top_20 / total_bads if total_bads > 0 else 0\n",
    "    \n",
    "    print(f\">>> REPORT FOR: {dataset_label}\")\n",
    "    print(f\"   ‚Ä¢ KS Statistic:       {ks_stat:.4f} ({(ks_stat*100):.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Brier Score:        {brier:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Capture Rate @ 10%: {capture_rate_10:.2%} (T√≥m ƒë∆∞·ª£c bao nhi√™u Bad trong top 10% r·ªßi ro nh·∫•t)\")\n",
    "    print(f\"   ‚Ä¢ Capture Rate @ 20%: {capture_rate_20:.2%}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return ks_stat, brier, capture_rate_10\n",
    "\n",
    "# ============================================================\n",
    "# H√ÄM T√çNH PSI (ƒê·ªò ·ªîN ƒê·ªäNH)\n",
    "# ============================================================\n",
    "def calculate_psi(expected, actual, buckets=10):\n",
    "    # expected: Ph√¢n ph·ªëi PD/Score t·∫≠p Train\n",
    "    # actual: Ph√¢n ph·ªëi PD/Score t·∫≠p OOT\n",
    "    \n",
    "    def scale_range(input, min, max):\n",
    "        input += -(np.min(input))\n",
    "        input /= np.max(input) / (max - min)\n",
    "        input += min\n",
    "        return input\n",
    "\n",
    "    breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "    \n",
    "    # Chia bin d·ª±a tr√™n t·∫≠p Train (Expected)\n",
    "    breakpoints = np.percentile(expected, breakpoints)\n",
    "    \n",
    "    # T√≠nh % s·ªë l∆∞·ª£ng m·∫´u trong t·ª´ng bin\n",
    "    expected_percents = np.histogram(expected, breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, breakpoints)[0] / len(actual)\n",
    "    \n",
    "    # X·ª≠ l√Ω division by zero\n",
    "    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)\n",
    "    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)\n",
    "    \n",
    "    # C√¥ng th·ª©c PSI\n",
    "    psi_value = np.sum((expected_percents - actual_percents) * np.log(expected_percents / actual_percents))\n",
    "    \n",
    "    return psi_value\n",
    "\n",
    "# ============================================================\n",
    "# CH·∫†Y B√ÅO C√ÅO SO S√ÅNH\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìä B√ÅO C√ÅO CH·ªà S·ªê N√ÇNG CAO CHO EBM\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. T√≠nh c√°c ch·ªâ s·ªë KS, Brier, Capture Rate tr√™n OOT\n",
    "# (Ch·ªâ s·ªë tr√™n OOT l√† quan tr·ªçng nh·∫•t ƒë·ªÉ so s√°nh c√¥ng b·∫±ng)\n",
    "calculate_advanced_metrics(ebm, X_test_ebm, y_test, \"OOT DATASET (T∆∞∆°ng lai)\")\n",
    "\n",
    "# 2. T√≠nh PSI (Train vs OOT)\n",
    "# D√πng x√°c su·∫•t d·ª± b√°o (PD) ƒë·ªÉ t√≠nh PSI\n",
    "prob_train = ebm.predict_proba(X_train_ebm)[:, 1]\n",
    "prob_oot   = ebm.predict_proba(X_test_ebm)[:, 1]\n",
    "\n",
    "psi_score = calculate_psi(prob_train, prob_oot)\n",
    "\n",
    "print(f\">>> STABILITY (PSI - Train vs OOT): {psi_score:.4f}\")\n",
    "if psi_score < 0.1:\n",
    "    print(\"   ‚úÖ PSI < 0.1: M√¥ h√¨nh r·∫•t ·ªïn ƒë·ªãnh.\")\n",
    "elif psi_score < 0.25:\n",
    "    print(\"   ‚ö†Ô∏è PSI 0.1-0.25: M√¥ h√¨nh bi·∫øn ƒë·ªông nh·∫π.\")\n",
    "else:\n",
    "    print(\"   ‚ùå PSI > 0.25: M√¥ h√¨nh kh√¥ng ·ªïn ƒë·ªãnh (C·∫£nh b√°o ƒë·ªè).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 4. OPTUNA TUNING ‚Äì EBM OPTIMIZED\n",
    "# # ============================================================\n",
    "# from optuna.pruners import HyperbandPruner\n",
    "# print(\"\\n>>> [3/6] OPTIMIZING MODEL WITH OPTUNA...\")\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05),\n",
    "#         \"interactions\": trial.suggest_int(\"interactions\", 0, 20),\n",
    "#         \"max_bins\": trial.suggest_int(\"max_bins\", 128, 512),\n",
    "#         \"outer_bags\": trial.suggest_int(\"outer_bags\", 4, 20),\n",
    "#         \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 100),\n",
    "#     }\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     gini_scores = []\n",
    "\n",
    "#     for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "#         X_tr, X_va = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "#         y_tr, y_va = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "#         model = ExplainableBoostingClassifier(\n",
    "#             monotone_constraints=mono_constraints_list,\n",
    "#             random_state=42,\n",
    "#             n_jobs=-1,\n",
    "#             **params\n",
    "#         )\n",
    "\n",
    "#         model.fit(X_tr, y_tr)\n",
    "\n",
    "#         prob = model.predict_proba(X_va)[:, 1]\n",
    "#         auc = roc_auc_score(y_va, prob)\n",
    "#         gini_scores.append(2*auc - 1)\n",
    "\n",
    "#     return np.mean(gini_scores)\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\",\n",
    "#     pruner=optuna.pruners.MedianPruner(\n",
    "#         n_startup_trials=5,\n",
    "#         n_warmup_steps=1,\n",
    "#         interval_steps=1\n",
    "\n",
    "#     ))\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# print(\"\\n>>> BEST PARAMETERS FOUND:\")\n",
    "# print(study.best_params)\n",
    "# print(f\"Best Gini (OOS): {study.best_value:.4f}\")\n",
    "\n",
    "# # Train l·∫°i model optimized\n",
    "# ebm_opt = ExplainableBoostingClassifier(\n",
    "#     monotone_constraints=mono_constraints_list,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "#     **study.best_params\n",
    "# )\n",
    "# ebm_opt.fit(X_train, y_train)\n",
    "\n",
    "# print(\"\\n>>> [4/6] GINI ‚Äì EBM OPTIMIZED\")\n",
    "# gini_opt_oos = calculate_gini(ebm_opt, X_val, y_val, \"OOS-Optimized\")\n",
    "# gini_opt_oot = calculate_gini(ebm_opt, X_test, y_test, \"OOT-Optimized\")\n",
    "\n",
    "# # Visualization\n",
    "# fig1 = plot_optimization_history(study)\n",
    "# fig2 = plot_param_importances(study)\n",
    "# fig1.show()\n",
    "# fig2.show()\n",
    "\n",
    "# # Save model\n",
    "# pickle.dump(ebm_opt, open(\"ebm_optimized.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna.visualization as vis\n",
    "\n",
    "# # 1. V·∫Ω Slice Plot cho t·∫•t c·∫£ c√°c tham s·ªë\n",
    "# fig_slice = vis.plot_slice(study)\n",
    "# fig_slice.show()\n",
    "\n",
    "# # 2. Ho·∫∑c v·∫Ω Slice Plot cho m·ªôt v√†i tham s·ªë quan tr·ªçng b·∫°n mu·ªën ƒë∆∞a v√†o b√†i vi·∫øt\n",
    "# fig_specific = vis.plot_slice(study, params=[\"learning_rate\", \"min_samples_leaf\", \"interactions\"])\n",
    "# fig_specific.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train l·∫°i model optimized\n",
    "# ebm_opt = ExplainableBoostingClassifier(\n",
    "#     monotone_constraints=mono_constraints_list,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "#     learning_rate=0.041459457759190334, \n",
    "#     interactions=8, \n",
    "#     max_bins=325, \n",
    "#     outer_bags=11, \n",
    "#     min_samples_leaf=27\n",
    "# )\n",
    "# ebm_opt.fit(X_train, y_train)\n",
    "\n",
    "# print(\"\\n>>> [4/6] GINI ‚Äì EBM OPTIMIZED\")\n",
    "# gini_opt_oos = calculate_gini(ebm_opt, X_val, y_val, \"OOS-Optimized\")\n",
    "# gini_opt_oot = calculate_gini(ebm_opt, X_test, y_test, \"OOT-Optimized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(ebm_opt, open(\"ebm_optimized.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 5. MODEL EXPLAINABILITY\n",
    "# # ============================================================\n",
    "\n",
    "# print(\"\\n>>> [5/6] EXPLAINABILITY REPORT...\")\n",
    "\n",
    "# # 5.1. GLOBAL FEATURE IMPORTANCE\n",
    "# global_importance = ebm_opt.explain_global()\n",
    "\n",
    "# print(\"\\n>>> TOP FEATURES (Global Importance):\")\n",
    "# for i, (name, score) in enumerate(zip(\n",
    "#         global_importance.data()['names'],\n",
    "#         global_importance.data()['scores'])):\n",
    "#     print(f\"{i+1:2d}. {name}: {score:.4f}\")\n",
    "\n",
    "# # 5.2. HI·ªÇN TH·ªä B·∫∞NG TR√åNH DUY·ªÜT (n·∫øu ch·∫°y trong notebook th√¨ hi·ªán ngay)\n",
    "# try:\n",
    "#     show(global_importance)\n",
    "# except:\n",
    "#     print(\"   (Kh√¥ng th·ªÉ render giao di·ªán tr·ª±c ti·∫øp ‚Äî v·∫´n ti·∫øp t·ª•c.)\")\n",
    "\n",
    "# # 5.3. LOCAL EXPLANATION (gi·∫£i th√≠ch 1 kh√°ch h√†ng b·∫•t k·ª≥)\n",
    "# sample_index = 0 \n",
    "# sample_X = X_test.iloc[[0]]  # v·∫´n l√† DataFrame 1 h√†ng\n",
    "# sample_y = y_test.iloc[0]    # Series scalar, 1D\n",
    "\n",
    "# local_exp = ebm_opt.explain_local(X_test, y_test)\n",
    "#   # Series\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n>>> LOCAL EXPLANATION SAMPLE:\")\n",
    "# show(local_exp)\n",
    "# print(local_exp.data(0))\n",
    "\n",
    "\n",
    "# try:\n",
    "#     importances = ebm_opt.term_importances()\n",
    "# except:\n",
    "#     try:\n",
    "#         importances = ebm_opt.feature_importances_\n",
    "#     except:\n",
    "#         raise ValueError(\"Kh√¥ng t√¨m th·∫•y thu·ªôc t√≠nh term_importances ho·∫∑c feature_importances.\")\n",
    "\n",
    "# terms = ebm_opt.term_names_\n",
    "\n",
    "# print(\"\\n>>> TOP INTERACTIONS:\")\n",
    "# for name, imp in sorted(zip(terms, importances), key=lambda x: x[1], reverse=True):\n",
    "#     if \"&\" in name:  # interaction lu√¥n c√≥ k√Ω t·ª± &\n",
    "#         print(f\"{name}: {imp:.4f}\")\n",
    "\n",
    "\n",
    "# # L∆∞u ph·∫ßn explain v√†o file\n",
    "# pickle.dump(global_importance, open(\"ebm_global_exp.pkl\", \"wb\"))\n",
    "# pickle.dump(local_exp, open(\"ebm_local_exp.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026eeb2b",
   "metadata": {},
   "source": [
    "Global importance: \n",
    "- Gi√° tr·ªã tuy·ªát ƒë·ªëi trung b√¨nh c·ªßa ƒëi·ªÉm s·ªë (Average Absolute Score) m√† bi·∫øn ƒë√≥ ƒë√≥ng g√≥p v√†o d·ª± b√°o tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu: Trung b√¨nh, bi·∫øn n√†y l√†m thay ƒë·ªïi d·ª± ƒëo√°n (log-odds) bao nhi√™u ƒë∆°n v·ªã? S·ªë c√†ng l·ªõn = Bi·∫øn c√†ng quan tr·ªçng.\n",
    "\n",
    "C√°ch t√≠nh, v√≠ d·ª• v·ªõi bi·∫øn SOHUUNHA:\n",
    "- B∆∞·ªõc 1: T√≠nh Score cho t·ª´ng ng∆∞·ªùi: V·ªõi m·ªói kh√°ch h√†ng trong t·∫≠p d·ªØ li·ªáu, m√¥ h√¨nh xem kh√°ch h√†ng ƒë√≥ c√≥ nh√† hay kh√¥ng (SOHUUNHA = 1 ho·∫∑c 0). Sau ƒë√≥, n√≥ tra c·ª©u trong b·∫£ng h√†m h√¨nh d·∫°ng xem gi√° tr·ªã ƒë√≥ t∆∞∆°ng ·ª©ng v·ªõi bao nhi√™u ƒëi·ªÉm (v√≠ d·ª•: C√≥ nh√† th√¨ $-0.8$ ƒëi·ªÉm, kh√¥ng nh√† th√¨ $+1.3$ ƒëi·ªÉm r·ªßi ro).\n",
    "\n",
    "- B∆∞·ªõc 2: L·∫•y tr·ªã tuy·ªát ƒë·ªëi: V√¨ m√¥ h√¨nh mu·ªën ƒëo l∆∞·ªùng m·ª©c ƒë·ªô t√°c ƒë·ªông (d√π l√† l√†m tƒÉng hay gi·∫£m r·ªßi ro ƒë·ªÅu ƒë∆∞·ª£c coi l√† quan tr·ªçng), n√™n n√≥ l·∫•y tr·ªã tuy·ªát ƒë·ªëi c·ªßa t·∫•t c·∫£ c√°c ƒëi·ªÉm s·ªë ƒë√≥: $|-0.8| = 0.8$ v√† $|1.3| = 1.3$.\n",
    "\n",
    "- B∆∞·ªõc 3: T√≠nh trung b√¨nh: C·ªông t·∫•t c·∫£ c√°c tr·ªã tuy·ªát ƒë·ªëi n√†y l·∫°i v√† chia cho t·ªïng s·ªë kh√°ch h√†ng. K·∫øt qu·∫£ cu·ªëi c√πng ch√≠nh l√† con s·ªë b·∫°n th·∫•y.\n",
    "\n",
    "T√°c ƒë·ªông c·ªßa bi·∫øn SOHUUNHA: \n",
    "- SOHUUNHA ‚âà 0 (Kh√¥ng s·ªü h·ªØu nh√†): * Gi√° tr·ªã Score n·∫±m ·ªü m·ª©c kho·∫£ng +0.8. Nh·ªØng kh√°ch h√†ng kh√¥ng c√≥ nh√† ri√™ng c√≥ xu h∆∞·ªõng l√†m tƒÉng x√°c su·∫•t c·ªßa bi·∫øn m·ª•c ti√™u\n",
    "- SOHUUNHA ‚âà 1 (C√≥ s·ªü h·ªØu nh√†): * Gi√° tr·ªã Score gi·∫£m m·∫°nh xu·ªëng m·ª©c kho·∫£ng -1.5. Vi·ªác s·ªü h·ªØu nh√† ƒë√≥ng g√≥p m·ªôt gi√° tr·ªã √¢m r·∫•t l·ªõn v√†o t·ªïng ƒëi·ªÉm. ƒêi·ªÅu n√†y l√†m gi·∫£m m·∫°nh x√°c su·∫•t x·∫£y ra bi·∫øn m·ª•c ti√™u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9be502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 6. FINAL REPORT ‚Äì PSI, COMPARISON, SUMMARY\n",
    "# # ============================================================\n",
    "\n",
    "# print(\"\\n>>> [6/6] FINAL MODEL REPORT...\")\n",
    "\n",
    "# # 6.1. T√≠nh PSI c·ªßa 2 bi·∫øn ch√≠nh gi·ªØa Train ‚Äì OOT\n",
    "# psi_report = {}\n",
    "# for col in [\"INCOME\", \"CBAL\", \"RATIO_DTI\", \"RATIO_UTILIZATION\"]:\n",
    "#     try:\n",
    "#         psi_report[col] = calculate_psi(\n",
    "#             X_train[col].values,\n",
    "#             X_test[col].values\n",
    "#         )\n",
    "#     except:\n",
    "#         psi_report[col] = None\n",
    "\n",
    "# print(\"\\n>>> PSI REPORT (Train ‚Üí OOT):\")\n",
    "# for k,v in psi_report.items():\n",
    "#     print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "# # 6.2. So s√°nh Gini Baseline vs Optimized\n",
    "# print(\"\\n>>> GINI COMPARISON\")\n",
    "# print(f\"Baseline ‚Äì OOS: {gini_base_oos:.4f}\")\n",
    "# print(f\"Optimized ‚Äì OOS: {gini_opt_oos:.4f}\")\n",
    "# print(f\"Baseline ‚Äì OOT: {gini_base_oot:.4f}\")\n",
    "# print(f\"Optimized ‚Äì OOT: {gini_opt_oot:.4f}\")\n",
    "\n",
    "# # 6.3. L∆∞u b·∫£n summary ra file TXT\n",
    "# # Best Hyperparameters:\n",
    "# # {study.best_params}\n",
    "# # \"\"\"\n",
    "# summary_text = f\"\"\"\n",
    "# =============================\n",
    "# EBM CREDIT SCORING SUMMARY\n",
    "# =============================\n",
    "\n",
    "# GINI ‚Äì OOS:\n",
    "#  - Baseline:  {gini_base_oos:.4f}\n",
    "#  - Optimized: {gini_opt_oos:.4f}\n",
    "\n",
    "# GINI ‚Äì OOT:\n",
    "#  - Baseline:  {gini_base_oot:.4f}\n",
    "#  - Optimized: {gini_opt_oot:.4f}\n",
    "\n",
    "# PSI (Train ‚Üí OOT):\n",
    "# {psi_report}\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# with open(\"model_summary.txt\", \"w\") as f:\n",
    "#     f.write(summary_text)\n",
    "\n",
    "# print(\"\\n>>> SUMMARY EXPORTED ‚Üí model_summary.txt\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac64cb7",
   "metadata": {},
   "source": [
    "M√¥ h√¨nh r·∫•t ·ªïn ƒë·ªãnh khi chuy·ªÉn t·ª´ TRAIN ‚Üí OOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6.4. T√≠nh PSI cho Score (X√°c su·∫•t d·ª± b√°o) - QUAN TR·ªåNG NH·∫§T\n",
    "# prob_train = ebm_opt.predict_proba(X_train)[:, 1]\n",
    "# prob_oot = ebm_opt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# psi_score = calculate_psi(prob_train, prob_oot)\n",
    "# print(f\"\\n>>> FINAL MODEL STABILITY (PSI Score): {psi_score:.4f}\")\n",
    "# if psi_score < 0.1:\n",
    "#     print(\"    => K·∫øt lu·∫≠n: M√¥ h√¨nh c·ª±c k·ª≥ ·ªïn ƒë·ªãnh qua th·ªùi gian.\")\n",
    "# elif psi_score < 0.25:\n",
    "#     print(\"    => K·∫øt lu·∫≠n: M√¥ h√¨nh c√≥ s·ª± bi·∫øn ƒë·ªông nh·∫π, c·∫ßn gi√°m s√°t.\")\n",
    "# else:\n",
    "#     print(\"    => K·∫øt lu·∫≠n: C·∫¢NH B√ÅO! M√¥ h√¨nh kh√¥ng ·ªïn ƒë·ªãnh, c·∫ßn ki·ªÉm tra l·∫°i d·ªØ li·ªáu OOT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# # ============================================================\n",
    "# # 7. N√ÇNG CAO: MONOTONICITY, CONFUSION MATRIX & RATING\n",
    "# # ============================================================\n",
    "# print(\"\\n>>> [7/7] ADVANCED ANALYTICS & RATING...\")\n",
    "\n",
    "# # 7.1. Ki·ªÉm tra t√≠nh ƒë∆°n ƒëi·ªáu (Monotonicity Check)\n",
    "# # EBM l∆∞u c√°c h√†m ƒë√≥ng g√≥p trong explain_global. \n",
    "# # B·∫°n c√≥ th·ªÉ v·∫Ω l·∫°i ƒë·ªÉ ki·ªÉm tra xem c√°c bi·∫øn √©p ƒë∆°n ƒëi·ªáu c√≥ ƒë√∫ng ƒë∆∞·ªùng th·∫≥ng/b·∫≠c thang kh√¥ng.\n",
    "# def check_monotonicity(ebm_model, feature_name):\n",
    "#     exp = ebm_model.explain_global(name=feature_name)\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     plt.step(exp.data(0)['names'], exp.data(0)['scores'], where='post')\n",
    "#     plt.title(f\"Monotonicity Check: {feature_name}\")\n",
    "#     plt.xlabel(feature_name)\n",
    "#     plt.ylabel(\"Logit Contribution\")\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# # Ch·∫°y th·ª≠ cho 2 bi·∫øn ti√™u bi·ªÉu\n",
    "# # check_monotonicity(ebm_opt, 'MAX_DPD_12M_OBS') \n",
    "# # check_monotonicity(ebm_opt, 'LTV')\n",
    "\n",
    "# # 7.2. Ma tr·∫≠n nh·∫ßm l·∫´n t·∫°i ƒëi·ªÉm c·∫Øt (Cut-off)\n",
    "# # Gi·∫£ s·ª≠ ng√¢n h√†ng ch·ªçn Cut-off l√† 10% (PD > 0.1 l√† t·ª´ ch·ªëi)\n",
    "# threshold = 0.1\n",
    "# y_prob_oot = ebm_opt.predict_proba(X_test)[:, 1]\n",
    "# y_pred_oot = (y_prob_oot >= threshold).astype(int)\n",
    "\n",
    "# print(f\"\\n>>> CONFUSION MATRIX (At Threshold {threshold}):\")\n",
    "# cm = confusion_matrix(y_test, y_pred_oot)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Good (0)', 'Bad (1)'])\n",
    "# disp.plot(cmap='Blues')\n",
    "# plt.title(f\"Confusion Matrix at Cut-off {threshold}\")\n",
    "# plt.show()\n",
    "\n",
    "# print(classification_report(y_test, y_pred_oot))\n",
    "\n",
    "# # 7.3. Ph√¢n h·∫°ng kh√°ch h√†ng (Credit Rating)\n",
    "# def assign_rating(pd):\n",
    "#     if pd <= 0.01: return 'AAA (R·∫•t an to√†n)'\n",
    "#     if pd <= 0.03: return 'AA'\n",
    "#     if pd <= 0.05: return 'A'\n",
    "#     if pd <= 0.10: return 'BBB'\n",
    "#     if pd <= 0.20: return 'BB'\n",
    "#     if pd <= 0.50: return 'B'\n",
    "#     return 'C (R·ªßi ro cao - T·ª´ ch·ªëi)'\n",
    "\n",
    "# # T·∫°o b·∫£ng k·∫øt qu·∫£ cu·ªëi c√πng cho t·∫≠p Test\n",
    "# df_result = pd.DataFrame({\n",
    "#     'Actual': y_test.values,\n",
    "#     'PD': y_prob_oot\n",
    "# })\n",
    "\n",
    "# df_result['Rating'] = df_result['PD'].apply(assign_rating)\n",
    "\n",
    "# print(\"\\n>>> TH·ªêNG K√ä H·∫†NG KH√ÅCH H√ÄNG TR√äN T·∫¨P OOT:\")\n",
    "# rating_dist = df_result['Rating'].value_counts().sort_index()\n",
    "# print(rating_dist)\n",
    "\n",
    "# # T√≠nh t·ª∑ l·ªá n·ª£ x·∫•u th·ª±c t·∫ø tr√™n m·ªói h·∫°ng (ƒê·ªÉ ki·ªÉm tra t√≠nh ph√¢n t√°ch)\n",
    "# bad_rate_by_rating = df_result.groupby('Rating')['Actual'].mean()\n",
    "# print(\"\\n>>> T·ª∂ L·ªÜ N·ª¢ X·∫§U TH·ª∞C T·∫æ THEO H·∫†NG (Bad Rate by Grade):\")\n",
    "# print(bad_rate_by_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 8. CREDIT SCORECARD CALCULATION (300 - 850)\n",
    "# # ============================================================\n",
    "# print(\"\\n>>> [8/7] CONVERTING PD TO CREDIT SCORE (300-850)...\")\n",
    "\n",
    "# def calculate_credit_score(pd, pdo=20, base_score=600, base_odds=50):\n",
    "#     \"\"\"\n",
    "#     Chuy·ªÉn ƒë·ªïi PD sang Credit Score d·ª±a tr√™n c√¥ng th·ª©c chu·∫©n:\n",
    "#     Score = Offset + Factor * ln(odds)\n",
    "#     - pdo: Points to Double the Odds (th∆∞·ªùng l√† 20 ho·∫∑c 50)\n",
    "#     - base_score: ƒêi·ªÉm c∆° s·ªü (v√≠ d·ª• 600 ƒëi·ªÉm)\n",
    "#     - base_odds: T·ª∑ l·ªá odds t∆∞∆°ng ·ª©ng v·ªõi ƒëi·ªÉm c∆° s·ªü (v√≠ d·ª• 50:1)\n",
    "#     \"\"\"\n",
    "#     # Tr√°nh chia cho 0 ho·∫∑c log(0)\n",
    "#     pd = np.clip(pd, 0.0001, 0.9999)\n",
    "    \n",
    "#     factor = pdo / np.log(2)\n",
    "#     offset = base_score - factor * np.log(base_odds)\n",
    "    \n",
    "#     odds = (1 - pd) / pd\n",
    "#     score = offset + factor * np.log(odds)\n",
    "    \n",
    "#     return np.round(score).astype(int)\n",
    "\n",
    "# # √Åp d·ª•ng cho t·∫≠p Test (OOT)\n",
    "# df_result['Credit_Score'] = calculate_credit_score(df_result['PD'])\n",
    "\n",
    "# # Gi·ªõi h·∫°n ƒëi·ªÉm trong kho·∫£ng 300 - 850 (nh∆∞ chu·∫©n FICO)\n",
    "# df_result['Credit_Score'] = df_result['Credit_Score'].clip(300, 850)\n",
    "\n",
    "# print(\"\\n>>> TH·ªêNG K√ä ƒêI·ªÇM T√çN D·ª§NG TR√äN T·∫¨P OOT:\")\n",
    "# print(df_result['Credit_Score'].describe())\n",
    "\n",
    "# # V·∫Ω bi·ªÉu ƒë·ªì ph√¢n ph·ªëi ƒëi·ªÉm s·ªë\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(df_result['Credit_Score'], bins=50, color='skyblue', edgecolor='black')\n",
    "# plt.axvline(df_result['Credit_Score'].mean(), color='red', linestyle='dashed', linewidth=2, label='Average Score')\n",
    "# plt.title('Distribution of Credit Scores (OOT Set)')\n",
    "# plt.xlabel('Credit Score')\n",
    "# plt.ylabel('Number of Customers')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Ki·ªÉm tra s·ª± t∆∞∆°ng quan gi·ªØa H·∫°ng v√† ƒêi·ªÉm trung b√¨nh\n",
    "# print(\"\\n>>> ƒêI·ªÇM TRUNG B√åNH THEO H·∫†NG:\")\n",
    "# print(df_result.groupby('Rating')['Credit_Score'].mean().sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Khoa_luan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
